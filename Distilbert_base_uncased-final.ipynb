{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PCCraveiro/5IADT-Fase03-Grupo25/blob/main/Distilbert_base_uncased-final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 🚀 Tech Challenge: Fine-Tuning de LLM\n",
        "\n",
        "&emsp; Este notebook implementa um pipeline completo de fine-tuning para modelos de linguagem (LLMs) no Google Colab.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Turma 5IADT | Grupo 25**\n",
        "\n",
        "🧑 Diego Henrique Silva - RM361935 - diegosource@gmail.com <br>\n",
        "🧑 Leandro Henrique Cavalcanti Bernardes - RM362274 - leandro.bernardes@hotmail.com <br>\n",
        "🧑 Paulo César Craveiro - RM363961 - pccraveiro@gmail.com <br>\n",
        "🧑 Reynaldo Teixeira Santos - RM360956 - reynaldots@gmail.com <br>\n",
        "🧑 Rodrigo Mendonca de Souza - RM364563 - rodrigo@volus.com <br>\n",
        " <br>\n",
        "\n",
        "## 🎯 Objetivos\n",
        "\n",
        "&emsp; Executar o fine-tuning de um foundation model utilizando o dataset \"The AmazonTitles-1.3MM\".\n",
        " <br>\n",
        " <br>\n",
        "📚 🎯 Modelo\n",
        "\n",
        "&emsp; Utilizamos o modelo **FLAN-T5**.\n",
        " <br>\n",
        " <br>\n",
        "\n",
        "📚 🖥 Ambiente\n",
        "\n",
        "&emsp; Utilizamos **GPU A100** que oferecem bom espaço (40GB VRAM).\n",
        " <br>\n",
        " <br>\n",
        "\n",
        "## ⚡ Etapas\n",
        "1.   Configurar ambiente.\n",
        "2.   Importar modelo pré-treinado.\n",
        "3.   Pré-processar Dataset.\n",
        "4.   Função e Massa de teste.\n",
        "5.   Teste com modelo pré-treinamento.\n",
        "6.   Fine-Tuning.\n",
        "7.   Teste com modelo pós-treinamento.\n",
        "8.   Resultados.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D6uHmn6TV2fU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tl1HpS6IEtiI"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 1. INSTALAÇÃO DE DEPENDÊNCIAS\n",
        "# ==============================================================================\n",
        "!pip install --upgrade pip -q\n",
        "!pip install --upgrade \\\n",
        "    torch torchvision torchaudio \\\n",
        "    transformers datasets tokenizers accelerate \\\n",
        "    scikit-learn pandas tqdm google-colab \\\n",
        "    gdown evaluate -q\n",
        "\n",
        "# Forçando o pyarrow para uma versão compatível:\n",
        "!pip install pyarrow==19.0.0 -q\n",
        "\n",
        "print(\"✅ Dependências instaladas com sucesso!\")\n",
        "print(\" --> O alerta de incompatibilidade de versões do 'PyArrow' não impacta em nada a execução desse Notbook.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. IMPORTS CONSOLIDADOS\n",
        "# ==============================================================================\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Para uso eficiente de convoluções e operações de GPU\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import gdown\n",
        "import zipfile\n",
        "import gzip\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import json\n",
        "import evaluate\n",
        "\n",
        "from datasets import ClassLabel\n",
        "from datasets import Dataset\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    DistilBertTokenizer, DistilBertForSequenceClassification,\n",
        "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
        ")\n",
        "from transformers.trainer_callback import EarlyStoppingCallback\n",
        "\n",
        "# Google Colab\n",
        "from google.colab import drive\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. CONEXÃO COM GOOGLE DRIVE\n",
        "# ==============================================================================\n",
        "# Conecta ao GoogleDrive para salvar os resultados e modelos.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX40bJsMFFGQ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 4. CONFIGURAÇÕES GLOBAIS E HIPERPARÂMETROS (AJUSTADO PARA BERT)\n",
        "# ==============================================================================\n",
        "# --- Configurações de Caminhos ---\n",
        "DATA_DIR = \"/content/drive/MyDrive/tech_challenge\"\n",
        "DRIVE_JSON_PATH = f\"{DATA_DIR}/trn.json\"\n",
        "LOCAL_JSON_PATH = \"./amazon_titles/LF-Amazon-1.3M/trn.json\"\n",
        "OUTPUT_DIR = \"bert_amz_titles\" # <-- MUDANÇA: Novo diretório de saída\n",
        "CACHE_DIR = \"./cache\"\n",
        "\n",
        "# --- Hiperparâmetros do Modelo e Treinamento ---\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "BATCH_SIZE = 768\n",
        "MAX_LENGTH = 384\n",
        "EPOCHS = 10\n",
        "LR = 3e-5\n",
        "WARMUP_STEPS = 500\n",
        "WEIGHT_DECAY = 0.01\n",
        "GRAD_ACC = 1\n",
        "FP16 = torch.cuda.is_available() # Ativa FP16 apenas se a GPU estiver disponível\n",
        "GRAD_CHECKPOINT = True\n",
        "WORKERS = 6                      # Número de workers para o dataloader\n",
        "VALIDATION_SPLIT = 0.2           # Usaremos uma divisão fixa de 20% para validação\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Tamanho da amostra de exemplo a ser usado (se zero pega o dataset inteiro).\n",
        "SAMPLES_EXAMPLES = 0\n",
        "\n",
        "# --- Controle de Semente para Reprodutibilidade ---\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04bAEOS3EqbZ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 5. FUNÇÕES AUXILIARES PARA DOWNLOAD E CARREGAMENTO\n",
        "# ==============================================================================\n",
        "\n",
        "def download_extract_file(extract_dir):\n",
        "    file_id = \"12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK\"\n",
        "    output = \"amazon_titles.zip\"\n",
        "    print(f\"📥 Iniciando download de {output}...\")\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output, quiet=False)\n",
        "\n",
        "    with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(f\"✅ ZIP descompactado em {extract_dir}\")\n",
        "\n",
        "    gz_path = os.path.join(extract_dir, \"LF-Amazon-1.3M\", \"trn.json.gz\")\n",
        "    json_path = os.path.join(extract_dir, \"LF-Amazon-1.3M\", \"trn.json\")\n",
        "\n",
        "    with gzip.open(gz_path, 'rt', encoding='utf-8') as f_in:\n",
        "        with open(json_path, 'w', encoding='utf-8') as f_out:\n",
        "            f_out.write(f_in.read())\n",
        "    print(f\"✅ trn.json.gz descompactado para {json_path}\")\n",
        "    return json_path\n",
        "\n",
        "def get_dataset_file(local_path, drive_path):\n",
        "    if os.path.isfile(local_path):\n",
        "        print(f\"✅ Arquivo já existe localmente: {local_path}\")\n",
        "        return local_path\n",
        "\n",
        "    folder = Path(local_path).parent\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if os.path.ismount(\"/content/drive\"):\n",
        "        print(\"📂 Google Drive está montado.\")\n",
        "        if os.path.isfile(drive_path):\n",
        "            print(\"📥 Copiando arquivo do Drive para o ambiente local...\")\n",
        "            shutil.copy(drive_path, local_path)\n",
        "            print(f\"✅ Copiado para {local_path}\")\n",
        "            return local_path\n",
        "        else:\n",
        "            print(\"❌ Arquivo não encontrado no Google Drive. Iniciando download.\")\n",
        "    else:\n",
        "        print(\"❌ Google Drive não está montado. Iniciando download.\")\n",
        "\n",
        "    return download_extract_file(folder)\n",
        "\n",
        "def load_amazontitles_json(path):\n",
        "    data = []\n",
        "    short_texts = 0\n",
        "\n",
        "    def clean_html_tags(text):\n",
        "        clean = re.compile('<.*?>')\n",
        "        return re.sub(clean, '', text)\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, desc=\"Lendo arquivo JSON bruto\"):\n",
        "            d = json.loads(line)\n",
        "            if all(field in d for field in [\"uid\", \"title\", \"content\", \"target_ind\"]):\n",
        "                title = (d[\"title\"] or \"\").strip()\n",
        "                content = (d[\"content\"] or \"\").strip()\n",
        "                target_ind = d[\"target_ind\"]\n",
        "                if not title or not content or not isinstance(target_ind, list) or len(target_ind) == 0:\n",
        "                    continue\n",
        "                label = target_ind[0]\n",
        "                combined_text = clean_html_tags(title) + \" [SEP] \" + clean_html_tags(content)\n",
        "                if len(combined_text.split()) < 5:\n",
        "                    short_texts += 1\n",
        "                    continue\n",
        "                data.append({\"text\": combined_text, \"raw_label\": label})\n",
        "\n",
        "    if short_texts > 0:\n",
        "        print(f\"Descartados {short_texts} exemplos por textos muito curtos.\")\n",
        "\n",
        "    return pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SSq1O-BMxm4"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 6. PIPELINE DE CARREGAMENTO E PRÉ-PROCESSAMENTO (COM AGRUPAMENTO DE CLASSES)\n",
        "# ==============================================================================\n",
        "\n",
        "# Garante que o arquivo de dados esteja disponível localmente\n",
        "local_file_path = get_dataset_file(LOCAL_JSON_PATH, DRIVE_JSON_PATH)\n",
        "\n",
        "print(\"\\nCarregando e processando dados...\")\n",
        "df = load_amazontitles_json(local_file_path)\n",
        "\n",
        "# Amostra de exemplos para um treinamento mais rápido (ajuste se necessário)\n",
        "if SAMPLES_EXAMPLES > 0:\n",
        "    print(f\"Usando uma amostra de {SAMPLES_EXAMPLES:,} exemplos do total de {len(df):,}.\")\n",
        "    df_filtered = df.sample(n=SAMPLES_EXAMPLES, random_state=RANDOM_SEED).dropna().reset_index(drop=True)\n",
        "else:\n",
        "    print(f\"Usando todos os exemplos -  total de {len(df):,}.\")\n",
        "    df_filtered = df.dropna().reset_index(drop=True)\n",
        "\n",
        "# Lógica de agrupamento de labels (binning) original\n",
        "# (Esta parte continua igual)\n",
        "num_general_classes = 25\n",
        "labels_sampled = df_filtered['raw_label'].tolist()\n",
        "min_label, max_label = min(labels_sampled), max(labels_sampled)\n",
        "df_filtered = df_filtered[(df_filtered['raw_label'] >= min_label) & (df_filtered['raw_label'] <= max_label)].copy()\n",
        "df_filtered['binned_label'] = pd.cut(df_filtered['raw_label'], bins=num_general_classes, labels=False, include_lowest=True)\n",
        "label_counts = df_filtered['binned_label'].value_counts()\n",
        "rare_labels = label_counts[label_counts == 1].index\n",
        "df_filtered = df_filtered[~df_filtered['binned_label'].isin(rare_labels)]\n",
        "unique_labels_binned = sorted(df_filtered['binned_label'].unique())\n",
        "label_map_binned = {old_label: new_label for new_label, old_label in enumerate(unique_labels_binned)}\n",
        "df_filtered['label'] = df_filtered['binned_label'].map(label_map_binned)\n",
        "\n",
        "print(f\"Agrupados {len(set(labels_sampled))} labels originais em {df_filtered['label'].nunique()} classes gerais.\")\n",
        "print(\"\\nDistribuição ANTES do agrupamento de classes raras:\")\n",
        "print(df_filtered['label'].value_counts().sort_index())\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# NOVA LÓGICA: AGRUPAMENTO DE CLASSES RARAS EM \"OUTROS\"\n",
        "# ==============================================================================\n",
        "print(\"\\nIniciando o agrupamento de classes raras...\")\n",
        "LIMIAR_CLASSE_RARA = 300 # Define que qualquer classe com menos de 300 exemplos é \"rara\"\n",
        "\n",
        "class_counts = df_filtered['label'].value_counts()\n",
        "classes_comuns = class_counts[class_counts >= LIMIAR_CLASSE_RARA].index.tolist()\n",
        "classes_raras = class_counts[class_counts < LIMIAR_CLASSE_RARA].index.tolist()\n",
        "\n",
        "# Mantém apenas as classes comuns e cria uma cópia para evitar warnings\n",
        "df_agrupado = df_filtered[df_filtered['label'].isin(classes_comuns)].copy()\n",
        "\n",
        "# Cria um dataframe com as classes raras e atribui a elas um novo label \"Outros\"\n",
        "if classes_raras:\n",
        "    label_outros = len(classes_comuns) # O novo label será o próximo número disponível\n",
        "    df_raras = df_filtered[df_filtered['label'].isin(classes_raras)].copy()\n",
        "    df_raras['label'] = label_outros\n",
        "    # Combina os dataframes\n",
        "    df_final = pd.concat([df_agrupado, df_raras], ignore_index=True)\n",
        "    print(f\"{len(classes_raras)} classes raras foram agrupadas na nova classe 'Outros' (label {label_outros}).\")\n",
        "else:\n",
        "    df_final = df_agrupado\n",
        "    print(\"Nenhuma classe rara encontrada para agrupar.\")\n",
        "\n",
        "\n",
        "# Remapeia todos os labels para serem contínuos (0, 1, 2, ...)\n",
        "# Isso é importante para o modelo\n",
        "unique_labels_final = sorted(df_final['label'].unique())\n",
        "final_label_map = {old_label: new_label for new_label, old_label in enumerate(unique_labels_final)}\n",
        "df_final['label'] = df_final['label'].map(final_label_map)\n",
        "\n",
        "# Atualiza a contagem final de classes\n",
        "n_classes = df_final['label'].nunique()\n",
        "print(f\"\\nO problema foi simplificado para {n_classes} classes no total.\")\n",
        "\n",
        "print(\"\\nDistribuição FINAL das classes (após agrupamento):\")\n",
        "print(df_final['label'].value_counts().sort_index())\n",
        "\n",
        "# Converte para o formato da biblioteca `datasets`\n",
        "hf_dataset = Dataset.from_pandas(df_final[['text', 'label']])\n",
        "\n",
        "# Converte a coluna 'label' para o tipo ClassLabel para permitir a estratificação\n",
        "hf_dataset = hf_dataset.cast_column(\"label\", ClassLabel(num_classes=n_classes))\n",
        "\n",
        "# ** IMPORTANTE: A variável df_filtered será substituída pela nova, que contém os dados agrupados **\n",
        "df_filtered = df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P_n_-4AcXQ2"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 7. TOKENIZAÇÃO E PREPARAÇÃO FINAL DO DATASET\n",
        "# ==============================================================================\n",
        "print(\"\\nInicializando o tokenizador...\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Função para tokenizar um lote de textos.\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "print(\"Aplicando tokenização ao dataset (pode levar alguns minutos)...\")\n",
        "# O método .map é altamente otimizado e processa os dados em lote\n",
        "tokenized_dataset = hf_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=1024,\n",
        "    num_proc=6,\n",
        "    load_from_cache_file=False)\n",
        "\n",
        "print(f\"\\nDividindo os dados em {1-VALIDATION_SPLIT:.0%} para treino e {VALIDATION_SPLIT:.0%} para validação...\")\n",
        "split_dataset = tokenized_dataset.train_test_split(\n",
        "    test_size=VALIDATION_SPLIT,\n",
        "    stratify_by_column='label',\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "train_dataset = split_dataset['train']\n",
        "val_dataset = split_dataset['test']\n",
        "\n",
        "print(f\"Tamanho final -> Treino: {len(train_dataset):,}, Validação: {len(val_dataset):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oFf5IS_11DI"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 8. TREINAMENTO DO MODELO COM BERT E BALANCEAMENTO DE CLASSES\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Define device (GPU or CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Calcular os Pesos das Classes ---\n",
        "train_labels = np.array(train_dataset['label'])\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "print(f\"Pesos calculados para as {n_classes} classes.\")\n",
        "\n",
        "# --- 2. Criar um Trainer Customizado para Usar os Pesos ---\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = CrossEntropyLoss(weight=class_weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# --- 3. Inicializar e Treinar o Modelo ---\n",
        "print(\"\\nInstanciando modelo BERT...\")\n",
        "# MUDANÇA: Usando a classe AutoModel...\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=n_classes,\n",
        "    cache_dir=CACHE_DIR\n",
        ")\n",
        "# MUDANÇA: O tokenizer também usa a classe AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Move o modelo para o dispositivo\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "if GRAD_CHECKPOINT:\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, y_true = eval_pred\n",
        "    y_pred = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "# Argumentos de Treinamento\n",
        "training_args = TrainingArguments(output_dir=OUTPUT_DIR, per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE,\n",
        "                                  gradient_accumulation_steps=GRAD_ACC, learning_rate=LR, num_train_epochs=EPOCHS, weight_decay=WEIGHT_DECAY,\n",
        "                                  warmup_steps=WARMUP_STEPS, eval_strategy=\"steps\", eval_steps=200, save_steps=200, logging_steps=25,\n",
        "                                  save_total_limit=2, fp16=FP16, dataloader_num_workers=WORKERS, dataloader_pin_memory=True,\n",
        "                                  load_best_model_at_end=True, metric_for_best_model=\"eval_loss\", greater_is_better=False,\n",
        "                                  gradient_checkpointing=GRAD_CHECKPOINT, report_to=\"none\")\n",
        "\n",
        "trainer = WeightedTrainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset,\n",
        "                          compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=5)])\n",
        "\n",
        "print(\"\\n🚀 Iniciando o treinamento com balanceamento de classes...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n✅ Treinamento concluído! Salvando o melhor modelo...\")\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"Modelo e tokenizador salvos em '{OUTPUT_DIR}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJV7z0Uu129Q"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 9. AVALIAÇÃO FINAL DETALHADA\n",
        "# ==============================================================================\n",
        "print(\"\\n📊 Executando avaliação final no conjunto de validação...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"\\nResultados das Métricas Finais:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"- {key}: {value:.4f}\")\n",
        "\n",
        "print(\"\\n🔎 Gerando previsões para análise detalhada...\")\n",
        "predictions = trainer.predict(val_dataset)\n",
        "\n",
        "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
        "y_true = predictions.label_ids\n",
        "\n",
        "# Gera nomes de classes para os relatórios\n",
        "target_names = [f'Classe {i}' for i in range(n_classes)]\n",
        "\n",
        "# --- Relatório de Classificação ---\n",
        "print(\"\\n📊 Relatório de Classificação (por classe):\")\n",
        "print(classification_report(y_true, y_pred, target_names=target_names, digits=3))\n",
        "\n",
        "# --- Matriz de Confusão ---\n",
        "print(\"\\n📊 Matriz de Confusão:\")\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
        "\n",
        "# Plotagem da matriz com tamanho ajustado\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "disp.plot(cmap=\"Blues\", xticks_rotation=90, values_format=\"d\", ax=ax)\n",
        "plt.title(\"Matriz de Confusão no Conjunto de Validação\")\n",
        "plt.show()\n",
        "\n",
        "# --- Histórico de Logs (Opcional) ---\n",
        "# print(\"\\n📜 Histórico completo de logs do treinamento:\")\n",
        "# print(trainer.state.log_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQNvTvW4AsnJ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 10. TESTE COM UMA NOVA PERGUNTA (CORRIGIDO COM AUTO CLASSES)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Defina sua pergunta de teste aqui ---\n",
        "pergunta_teste = \"4k monitor for gaming with high refresh rate\"\n",
        "\n",
        "# --- Mapeamento de ID para Label (para legibilidade da resposta) ---\n",
        "id2label = {i: f\"Classe Prevista {i}\" for i in range(n_classes)}\n",
        "\n",
        "# --- Verificação de dispositivo (GPU ou CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# Resposta 1: Modelo SEM Treinamento (BERT base)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🔎 1. PREVISÃO COM O MODELO ORIGINAL (SEM TREINAMENTO)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Carrega o modelo e tokenizador originais da Hugging Face usando as classes Auto\n",
        "tokenizer_base = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model_base = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=n_classes)\n",
        "model_base.to(device)\n",
        "\n",
        "# Prepara a pergunta\n",
        "inputs_base = tokenizer_base(pergunta_teste, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Realiza a previsão\n",
        "with torch.no_grad():\n",
        "    logits_base = model_base(**inputs_base).logits\n",
        "\n",
        "previsao_id_base = logits_base.argmax().item()\n",
        "previsao_label_base = id2label[previsao_id_base]\n",
        "\n",
        "print(f\"Texto da Pergunta: '{pergunta_teste}'\")\n",
        "print(f\"➡️ Resposta do Modelo SEM Treinamento: {previsao_label_base} (ID: {previsao_id_base})\\n\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Resposta 2: Modelo COM Treinamento (Carregado do seu diretório)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"🚀 2. PREVISÃO COM O SEU MODELO TREINADO\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# MUDANÇA: Carrega o SEU modelo e tokenizador salvos usando as classes Auto\n",
        "tokenizer_treinado = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "model_treinado = AutoModelForSequenceClassification.from_pretrained(OUTPUT_DIR)\n",
        "model_treinado.to(device)\n",
        "\n",
        "# Prepara a mesma pergunta\n",
        "inputs_treinado = tokenizer_treinado(pergunta_teste, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Realiza a previsão\n",
        "with torch.no_grad():\n",
        "    logits_treinado = model_treinado(**inputs_treinado).logits\n",
        "\n",
        "previsao_id_treinado = logits_treinado.argmax().item()\n",
        "previsao_label_treinado = id2label[previsao_id_treinado]\n",
        "\n",
        "print(f\"Texto da Pergunta: '{pergunta_teste}'\")\n",
        "print(f\"➡️ Resposta do Modelo COM Treinamento: {previsao_label_treinado} (ID: {previsao_id_treinado})\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}