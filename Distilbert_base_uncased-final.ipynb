{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PCCraveiro/5IADT-Fase03-Grupo25/blob/main/Distilbert_base_uncased-final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üöÄ Tech Challenge: Fine-Tuning de LLM\n",
        "\n",
        "&emsp; Este notebook implementa um pipeline completo de fine-tuning para modelos de linguagem (LLMs) no Google Colab.\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "**Turma 5IADT | Grupo 25**\n",
        "\n",
        "üßë Diego Henrique Silva - RM361935 - diegosource@gmail.com <br>\n",
        "üßë Leandro Henrique Cavalcanti Bernardes - RM362274 - leandro.bernardes@hotmail.com <br>\n",
        "üßë Paulo C√©sar Craveiro - RM363961 - pccraveiro@gmail.com <br>\n",
        "üßë Reynaldo Teixeira Santos - RM360956 - reynaldots@gmail.com <br>\n",
        "üßë Rodrigo Mendonca de Souza - RM364563 - rodrigo@volus.com <br>\n",
        " <br>\n",
        "\n",
        "## üéØ Objetivos\n",
        "\n",
        "&emsp; Executar o fine-tuning de um foundation model utilizando o dataset \"The AmazonTitles-1.3MM\".\n",
        " <br>\n",
        " <br>\n",
        "üìö üéØ Modelo\n",
        "\n",
        "&emsp; Utilizamos o modelo **FLAN-T5**.\n",
        " <br>\n",
        " <br>\n",
        "\n",
        "üìö üñ• Ambiente\n",
        "\n",
        "&emsp; Utilizamos **GPU A100** que oferecem bom espa√ßo (40GB VRAM).\n",
        " <br>\n",
        " <br>\n",
        "\n",
        "## ‚ö° Etapas\n",
        "1.   Configurar ambiente.\n",
        "2.   Importar modelo pr√©-treinado.\n",
        "3.   Pr√©-processar Dataset.\n",
        "4.   Fun√ß√£o e Massa de teste.\n",
        "5.   Teste com modelo pr√©-treinamento.\n",
        "6.   Fine-Tuning.\n",
        "7.   Teste com modelo p√≥s-treinamento.\n",
        "8.   Resultados.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "D6uHmn6TV2fU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tl1HpS6IEtiI"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 1. INSTALA√á√ÉO DE DEPEND√äNCIAS\n",
        "# ==============================================================================\n",
        "!pip install --upgrade pip -q\n",
        "!pip install --upgrade \\\n",
        "    torch torchvision torchaudio \\\n",
        "    transformers datasets tokenizers accelerate \\\n",
        "    scikit-learn pandas tqdm google-colab \\\n",
        "    gdown evaluate -q\n",
        "\n",
        "# For√ßando o pyarrow para uma vers√£o compat√≠vel:\n",
        "!pip install pyarrow==19.0.0 -q\n",
        "\n",
        "print(\"‚úÖ Depend√™ncias instaladas com sucesso!\")\n",
        "print(\" --> O alerta de incompatibilidade de vers√µes do 'PyArrow' n√£o impacta em nada a execu√ß√£o desse Notbook.\")\n",
        "\n",
        "# ==============================================================================\n",
        "# 2. IMPORTS CONSOLIDADOS\n",
        "# ==============================================================================\n",
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "# Para uso eficiente de convolu√ß√µes e opera√ß√µes de GPU\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import gdown\n",
        "import zipfile\n",
        "import gzip\n",
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "import re\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "from pathlib import Path\n",
        "from collections import Counter\n",
        "import json\n",
        "import evaluate\n",
        "\n",
        "from datasets import ClassLabel\n",
        "from datasets import Dataset\n",
        "\n",
        "# Hugging Face\n",
        "from transformers import (\n",
        "    AutoTokenizer, AutoModelForSequenceClassification,\n",
        "    DistilBertTokenizer, DistilBertForSequenceClassification,\n",
        "    TrainingArguments, Trainer, EarlyStoppingCallback\n",
        ")\n",
        "from transformers.trainer_callback import EarlyStoppingCallback\n",
        "\n",
        "# Google Colab\n",
        "from google.colab import drive\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# ==============================================================================\n",
        "# 3. CONEX√ÉO COM GOOGLE DRIVE\n",
        "# ==============================================================================\n",
        "# Conecta ao GoogleDrive para salvar os resultados e modelos.\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NX40bJsMFFGQ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 4. CONFIGURA√á√ïES GLOBAIS E HIPERPAR√ÇMETROS (AJUSTADO PARA BERT)\n",
        "# ==============================================================================\n",
        "# --- Configura√ß√µes de Caminhos ---\n",
        "DATA_DIR = \"/content/drive/MyDrive/tech_challenge\"\n",
        "DRIVE_JSON_PATH = f\"{DATA_DIR}/trn.json\"\n",
        "LOCAL_JSON_PATH = \"./amazon_titles/LF-Amazon-1.3M/trn.json\"\n",
        "OUTPUT_DIR = \"bert_amz_titles\" # <-- MUDAN√áA: Novo diret√≥rio de sa√≠da\n",
        "CACHE_DIR = \"./cache\"\n",
        "\n",
        "# --- Hiperpar√¢metros do Modelo e Treinamento ---\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "BATCH_SIZE = 768\n",
        "MAX_LENGTH = 384\n",
        "EPOCHS = 10\n",
        "LR = 3e-5\n",
        "WARMUP_STEPS = 500\n",
        "WEIGHT_DECAY = 0.01\n",
        "GRAD_ACC = 1\n",
        "FP16 = torch.cuda.is_available() # Ativa FP16 apenas se a GPU estiver dispon√≠vel\n",
        "GRAD_CHECKPOINT = True\n",
        "WORKERS = 6                      # N√∫mero de workers para o dataloader\n",
        "VALIDATION_SPLIT = 0.2           # Usaremos uma divis√£o fixa de 20% para valida√ß√£o\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Tamanho da amostra de exemplo a ser usado (se zero pega o dataset inteiro).\n",
        "SAMPLES_EXAMPLES = 0\n",
        "\n",
        "# --- Controle de Semente para Reprodutibilidade ---\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "04bAEOS3EqbZ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 5. FUN√á√ïES AUXILIARES PARA DOWNLOAD E CARREGAMENTO\n",
        "# ==============================================================================\n",
        "\n",
        "def download_extract_file(extract_dir):\n",
        "    file_id = \"12zH4mL2RX8iSvH0VCNnd3QxO4DzuHWnK\"\n",
        "    output = \"amazon_titles.zip\"\n",
        "    print(f\"üì• Iniciando download de {output}...\")\n",
        "    gdown.download(f\"https://drive.google.com/uc?id={file_id}\", output, quiet=False)\n",
        "\n",
        "    with zipfile.ZipFile(output, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_dir)\n",
        "    print(f\"‚úÖ ZIP descompactado em {extract_dir}\")\n",
        "\n",
        "    gz_path = os.path.join(extract_dir, \"LF-Amazon-1.3M\", \"trn.json.gz\")\n",
        "    json_path = os.path.join(extract_dir, \"LF-Amazon-1.3M\", \"trn.json\")\n",
        "\n",
        "    with gzip.open(gz_path, 'rt', encoding='utf-8') as f_in:\n",
        "        with open(json_path, 'w', encoding='utf-8') as f_out:\n",
        "            f_out.write(f_in.read())\n",
        "    print(f\"‚úÖ trn.json.gz descompactado para {json_path}\")\n",
        "    return json_path\n",
        "\n",
        "def get_dataset_file(local_path, drive_path):\n",
        "    if os.path.isfile(local_path):\n",
        "        print(f\"‚úÖ Arquivo j√° existe localmente: {local_path}\")\n",
        "        return local_path\n",
        "\n",
        "    folder = Path(local_path).parent\n",
        "    folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    if os.path.ismount(\"/content/drive\"):\n",
        "        print(\"üìÇ Google Drive est√° montado.\")\n",
        "        if os.path.isfile(drive_path):\n",
        "            print(\"üì• Copiando arquivo do Drive para o ambiente local...\")\n",
        "            shutil.copy(drive_path, local_path)\n",
        "            print(f\"‚úÖ Copiado para {local_path}\")\n",
        "            return local_path\n",
        "        else:\n",
        "            print(\"‚ùå Arquivo n√£o encontrado no Google Drive. Iniciando download.\")\n",
        "    else:\n",
        "        print(\"‚ùå Google Drive n√£o est√° montado. Iniciando download.\")\n",
        "\n",
        "    return download_extract_file(folder)\n",
        "\n",
        "def load_amazontitles_json(path):\n",
        "    data = []\n",
        "    short_texts = 0\n",
        "\n",
        "    def clean_html_tags(text):\n",
        "        clean = re.compile('<.*?>')\n",
        "        return re.sub(clean, '', text)\n",
        "\n",
        "    with open(path, 'r', encoding='utf-8') as f:\n",
        "        for line in tqdm(f, desc=\"Lendo arquivo JSON bruto\"):\n",
        "            d = json.loads(line)\n",
        "            if all(field in d for field in [\"uid\", \"title\", \"content\", \"target_ind\"]):\n",
        "                title = (d[\"title\"] or \"\").strip()\n",
        "                content = (d[\"content\"] or \"\").strip()\n",
        "                target_ind = d[\"target_ind\"]\n",
        "                if not title or not content or not isinstance(target_ind, list) or len(target_ind) == 0:\n",
        "                    continue\n",
        "                label = target_ind[0]\n",
        "                combined_text = clean_html_tags(title) + \" [SEP] \" + clean_html_tags(content)\n",
        "                if len(combined_text.split()) < 5:\n",
        "                    short_texts += 1\n",
        "                    continue\n",
        "                data.append({\"text\": combined_text, \"raw_label\": label})\n",
        "\n",
        "    if short_texts > 0:\n",
        "        print(f\"Descartados {short_texts} exemplos por textos muito curtos.\")\n",
        "\n",
        "    return pd.DataFrame(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2SSq1O-BMxm4"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 6. PIPELINE DE CARREGAMENTO E PR√â-PROCESSAMENTO (COM AGRUPAMENTO DE CLASSES)\n",
        "# ==============================================================================\n",
        "\n",
        "# Garante que o arquivo de dados esteja dispon√≠vel localmente\n",
        "local_file_path = get_dataset_file(LOCAL_JSON_PATH, DRIVE_JSON_PATH)\n",
        "\n",
        "print(\"\\nCarregando e processando dados...\")\n",
        "df = load_amazontitles_json(local_file_path)\n",
        "\n",
        "# Amostra de exemplos para um treinamento mais r√°pido (ajuste se necess√°rio)\n",
        "if SAMPLES_EXAMPLES > 0:\n",
        "    print(f\"Usando uma amostra de {SAMPLES_EXAMPLES:,} exemplos do total de {len(df):,}.\")\n",
        "    df_filtered = df.sample(n=SAMPLES_EXAMPLES, random_state=RANDOM_SEED).dropna().reset_index(drop=True)\n",
        "else:\n",
        "    print(f\"Usando todos os exemplos -  total de {len(df):,}.\")\n",
        "    df_filtered = df.dropna().reset_index(drop=True)\n",
        "\n",
        "# L√≥gica de agrupamento de labels (binning) original\n",
        "# (Esta parte continua igual)\n",
        "num_general_classes = 25\n",
        "labels_sampled = df_filtered['raw_label'].tolist()\n",
        "min_label, max_label = min(labels_sampled), max(labels_sampled)\n",
        "df_filtered = df_filtered[(df_filtered['raw_label'] >= min_label) & (df_filtered['raw_label'] <= max_label)].copy()\n",
        "df_filtered['binned_label'] = pd.cut(df_filtered['raw_label'], bins=num_general_classes, labels=False, include_lowest=True)\n",
        "label_counts = df_filtered['binned_label'].value_counts()\n",
        "rare_labels = label_counts[label_counts == 1].index\n",
        "df_filtered = df_filtered[~df_filtered['binned_label'].isin(rare_labels)]\n",
        "unique_labels_binned = sorted(df_filtered['binned_label'].unique())\n",
        "label_map_binned = {old_label: new_label for new_label, old_label in enumerate(unique_labels_binned)}\n",
        "df_filtered['label'] = df_filtered['binned_label'].map(label_map_binned)\n",
        "\n",
        "print(f\"Agrupados {len(set(labels_sampled))} labels originais em {df_filtered['label'].nunique()} classes gerais.\")\n",
        "print(\"\\nDistribui√ß√£o ANTES do agrupamento de classes raras:\")\n",
        "print(df_filtered['label'].value_counts().sort_index())\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# NOVA L√ìGICA: AGRUPAMENTO DE CLASSES RARAS EM \"OUTROS\"\n",
        "# ==============================================================================\n",
        "print(\"\\nIniciando o agrupamento de classes raras...\")\n",
        "LIMIAR_CLASSE_RARA = 300 # Define que qualquer classe com menos de 300 exemplos √© \"rara\"\n",
        "\n",
        "class_counts = df_filtered['label'].value_counts()\n",
        "classes_comuns = class_counts[class_counts >= LIMIAR_CLASSE_RARA].index.tolist()\n",
        "classes_raras = class_counts[class_counts < LIMIAR_CLASSE_RARA].index.tolist()\n",
        "\n",
        "# Mant√©m apenas as classes comuns e cria uma c√≥pia para evitar warnings\n",
        "df_agrupado = df_filtered[df_filtered['label'].isin(classes_comuns)].copy()\n",
        "\n",
        "# Cria um dataframe com as classes raras e atribui a elas um novo label \"Outros\"\n",
        "if classes_raras:\n",
        "    label_outros = len(classes_comuns) # O novo label ser√° o pr√≥ximo n√∫mero dispon√≠vel\n",
        "    df_raras = df_filtered[df_filtered['label'].isin(classes_raras)].copy()\n",
        "    df_raras['label'] = label_outros\n",
        "    # Combina os dataframes\n",
        "    df_final = pd.concat([df_agrupado, df_raras], ignore_index=True)\n",
        "    print(f\"{len(classes_raras)} classes raras foram agrupadas na nova classe 'Outros' (label {label_outros}).\")\n",
        "else:\n",
        "    df_final = df_agrupado\n",
        "    print(\"Nenhuma classe rara encontrada para agrupar.\")\n",
        "\n",
        "\n",
        "# Remapeia todos os labels para serem cont√≠nuos (0, 1, 2, ...)\n",
        "# Isso √© importante para o modelo\n",
        "unique_labels_final = sorted(df_final['label'].unique())\n",
        "final_label_map = {old_label: new_label for new_label, old_label in enumerate(unique_labels_final)}\n",
        "df_final['label'] = df_final['label'].map(final_label_map)\n",
        "\n",
        "# Atualiza a contagem final de classes\n",
        "n_classes = df_final['label'].nunique()\n",
        "print(f\"\\nO problema foi simplificado para {n_classes} classes no total.\")\n",
        "\n",
        "print(\"\\nDistribui√ß√£o FINAL das classes (ap√≥s agrupamento):\")\n",
        "print(df_final['label'].value_counts().sort_index())\n",
        "\n",
        "# Converte para o formato da biblioteca `datasets`\n",
        "hf_dataset = Dataset.from_pandas(df_final[['text', 'label']])\n",
        "\n",
        "# Converte a coluna 'label' para o tipo ClassLabel para permitir a estratifica√ß√£o\n",
        "hf_dataset = hf_dataset.cast_column(\"label\", ClassLabel(num_classes=n_classes))\n",
        "\n",
        "# ** IMPORTANTE: A vari√°vel df_filtered ser√° substitu√≠da pela nova, que cont√©m os dados agrupados **\n",
        "df_filtered = df_final"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-P_n_-4AcXQ2"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 7. TOKENIZA√á√ÉO E PREPARA√á√ÉO FINAL DO DATASET\n",
        "# ==============================================================================\n",
        "print(\"\\nInicializando o tokenizador...\")\n",
        "tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "def tokenize_function(examples):\n",
        "    \"\"\"Fun√ß√£o para tokenizar um lote de textos.\"\"\"\n",
        "    return tokenizer(\n",
        "        examples[\"text\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=MAX_LENGTH\n",
        "    )\n",
        "\n",
        "print(\"Aplicando tokeniza√ß√£o ao dataset (pode levar alguns minutos)...\")\n",
        "# O m√©todo .map √© altamente otimizado e processa os dados em lote\n",
        "tokenized_dataset = hf_dataset.map(\n",
        "    tokenize_function,\n",
        "    batched=True,\n",
        "    batch_size=1024,\n",
        "    num_proc=6,\n",
        "    load_from_cache_file=False)\n",
        "\n",
        "print(f\"\\nDividindo os dados em {1-VALIDATION_SPLIT:.0%} para treino e {VALIDATION_SPLIT:.0%} para valida√ß√£o...\")\n",
        "split_dataset = tokenized_dataset.train_test_split(\n",
        "    test_size=VALIDATION_SPLIT,\n",
        "    stratify_by_column='label',\n",
        "    seed=RANDOM_SEED\n",
        ")\n",
        "\n",
        "train_dataset = split_dataset['train']\n",
        "val_dataset = split_dataset['test']\n",
        "\n",
        "print(f\"Tamanho final -> Treino: {len(train_dataset):,}, Valida√ß√£o: {len(val_dataset):,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5oFf5IS_11DI"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 8. TREINAMENTO DO MODELO COM BERT E BALANCEAMENTO DE CLASSES\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Define device (GPU or CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "\n",
        "# --- 1. Calcular os Pesos das Classes ---\n",
        "train_labels = np.array(train_dataset['label'])\n",
        "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(train_labels), y=train_labels)\n",
        "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "print(f\"Pesos calculados para as {n_classes} classes.\")\n",
        "\n",
        "# --- 2. Criar um Trainer Customizado para Usar os Pesos ---\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "        loss_fct = CrossEntropyLoss(weight=class_weights_tensor)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# --- 3. Inicializar e Treinar o Modelo ---\n",
        "print(\"\\nInstanciando modelo BERT...\")\n",
        "# MUDAN√áA: Usando a classe AutoModel...\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=n_classes,\n",
        "    cache_dir=CACHE_DIR\n",
        ")\n",
        "# MUDAN√áA: O tokenizer tamb√©m usa a classe AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "\n",
        "# Move o modelo para o dispositivo\n",
        "model.to(device)\n",
        "\n",
        "\n",
        "if GRAD_CHECKPOINT:\n",
        "    model.gradient_checkpointing_enable()\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, y_true = eval_pred\n",
        "    y_pred = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(y_true, y_pred)\n",
        "    f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
        "    return {\"accuracy\": acc, \"f1\": f1}\n",
        "\n",
        "# Argumentos de Treinamento\n",
        "training_args = TrainingArguments(output_dir=OUTPUT_DIR, per_device_train_batch_size=BATCH_SIZE, per_device_eval_batch_size=BATCH_SIZE,\n",
        "                                  gradient_accumulation_steps=GRAD_ACC, learning_rate=LR, num_train_epochs=EPOCHS, weight_decay=WEIGHT_DECAY,\n",
        "                                  warmup_steps=WARMUP_STEPS, eval_strategy=\"steps\", eval_steps=200, save_steps=200, logging_steps=25,\n",
        "                                  save_total_limit=2, fp16=FP16, dataloader_num_workers=WORKERS, dataloader_pin_memory=True,\n",
        "                                  load_best_model_at_end=True, metric_for_best_model=\"eval_loss\", greater_is_better=False,\n",
        "                                  gradient_checkpointing=GRAD_CHECKPOINT, report_to=\"none\")\n",
        "\n",
        "trainer = WeightedTrainer(model=model, args=training_args, train_dataset=train_dataset, eval_dataset=val_dataset,\n",
        "                          compute_metrics=compute_metrics, callbacks=[EarlyStoppingCallback(early_stopping_patience=5)])\n",
        "\n",
        "print(\"\\nüöÄ Iniciando o treinamento com balanceamento de classes...\")\n",
        "trainer.train()\n",
        "\n",
        "print(\"\\n‚úÖ Treinamento conclu√≠do! Salvando o melhor modelo...\")\n",
        "trainer.save_model(OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR)\n",
        "print(f\"Modelo e tokenizador salvos em '{OUTPUT_DIR}'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZJV7z0Uu129Q"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 9. AVALIA√á√ÉO FINAL DETALHADA\n",
        "# ==============================================================================\n",
        "print(\"\\nüìä Executando avalia√ß√£o final no conjunto de valida√ß√£o...\")\n",
        "eval_results = trainer.evaluate()\n",
        "print(\"\\nResultados das M√©tricas Finais:\")\n",
        "for key, value in eval_results.items():\n",
        "    print(f\"- {key}: {value:.4f}\")\n",
        "\n",
        "print(\"\\nüîé Gerando previs√µes para an√°lise detalhada...\")\n",
        "predictions = trainer.predict(val_dataset)\n",
        "\n",
        "y_pred = np.argmax(predictions.predictions, axis=-1)\n",
        "y_true = predictions.label_ids\n",
        "\n",
        "# Gera nomes de classes para os relat√≥rios\n",
        "target_names = [f'Classe {i}' for i in range(n_classes)]\n",
        "\n",
        "# --- Relat√≥rio de Classifica√ß√£o ---\n",
        "print(\"\\nüìä Relat√≥rio de Classifica√ß√£o (por classe):\")\n",
        "print(classification_report(y_true, y_pred, target_names=target_names, digits=3))\n",
        "\n",
        "# --- Matriz de Confus√£o ---\n",
        "print(\"\\nüìä Matriz de Confus√£o:\")\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=target_names)\n",
        "\n",
        "# Plotagem da matriz com tamanho ajustado\n",
        "fig, ax = plt.subplots(figsize=(12, 12))\n",
        "disp.plot(cmap=\"Blues\", xticks_rotation=90, values_format=\"d\", ax=ax)\n",
        "plt.title(\"Matriz de Confus√£o no Conjunto de Valida√ß√£o\")\n",
        "plt.show()\n",
        "\n",
        "# --- Hist√≥rico de Logs (Opcional) ---\n",
        "# print(\"\\nüìú Hist√≥rico completo de logs do treinamento:\")\n",
        "# print(trainer.state.log_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQNvTvW4AsnJ"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# 10. TESTE COM UMA NOVA PERGUNTA (CORRIGIDO COM AUTO CLASSES)\n",
        "# ==============================================================================\n",
        "\n",
        "# --- Defina sua pergunta de teste aqui ---\n",
        "pergunta_teste = \"4k monitor for gaming with high refresh rate\"\n",
        "\n",
        "# --- Mapeamento de ID para Label (para legibilidade da resposta) ---\n",
        "id2label = {i: f\"Classe Prevista {i}\" for i in range(n_classes)}\n",
        "\n",
        "# --- Verifica√ß√£o de dispositivo (GPU ou CPU) ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# ==============================================================================\n",
        "# Resposta 1: Modelo SEM Treinamento (BERT base)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üîé 1. PREVIS√ÉO COM O MODELO ORIGINAL (SEM TREINAMENTO)\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Carrega o modelo e tokenizador originais da Hugging Face usando as classes Auto\n",
        "tokenizer_base = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "model_base = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=n_classes)\n",
        "model_base.to(device)\n",
        "\n",
        "# Prepara a pergunta\n",
        "inputs_base = tokenizer_base(pergunta_teste, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Realiza a previs√£o\n",
        "with torch.no_grad():\n",
        "    logits_base = model_base(**inputs_base).logits\n",
        "\n",
        "previsao_id_base = logits_base.argmax().item()\n",
        "previsao_label_base = id2label[previsao_id_base]\n",
        "\n",
        "print(f\"Texto da Pergunta: '{pergunta_teste}'\")\n",
        "print(f\"‚û°Ô∏è Resposta do Modelo SEM Treinamento: {previsao_label_base} (ID: {previsao_id_base})\\n\")\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# Resposta 2: Modelo COM Treinamento (Carregado do seu diret√≥rio)\n",
        "# ==============================================================================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üöÄ 2. PREVIS√ÉO COM O SEU MODELO TREINADO\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# MUDAN√áA: Carrega o SEU modelo e tokenizador salvos usando as classes Auto\n",
        "tokenizer_treinado = AutoTokenizer.from_pretrained(OUTPUT_DIR)\n",
        "model_treinado = AutoModelForSequenceClassification.from_pretrained(OUTPUT_DIR)\n",
        "model_treinado.to(device)\n",
        "\n",
        "# Prepara a mesma pergunta\n",
        "inputs_treinado = tokenizer_treinado(pergunta_teste, return_tensors=\"pt\").to(device)\n",
        "\n",
        "# Realiza a previs√£o\n",
        "with torch.no_grad():\n",
        "    logits_treinado = model_treinado(**inputs_treinado).logits\n",
        "\n",
        "previsao_id_treinado = logits_treinado.argmax().item()\n",
        "previsao_label_treinado = id2label[previsao_id_treinado]\n",
        "\n",
        "print(f\"Texto da Pergunta: '{pergunta_teste}'\")\n",
        "print(f\"‚û°Ô∏è Resposta do Modelo COM Treinamento: {previsao_label_treinado} (ID: {previsao_id_treinado})\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}