### Resumo do Fine‑Tuning

- Treinamento completou 1 época sobre 50.000 exemplos em 673 passos.  
- Loss inicial ~3.44, final médio perto de 3.22 com flutuações leves ao longo da época.  
- Geração avaliada com max_new_tokens=272.  
- Métricas comparativas mostram melhoria clara em ROUGE‑1 e ROUGE‑L, ROUGE‑2 está estável, BLEU passa de 0.0000 para 0.0007.

---

### Dinâmica do Treinamento

- **Tendência**: perda decrescente e relativamente estável, sem explosão ou NaNs, indicando aprendizado real.  
- **Velocidade de melhoria**: lenta — redução de ~0.22 em loss ao longo de 673 steps. Isso é esperado para uma única época em dataset grande.  
- **Oscilações**: pequenas flutuações indicam ruído do gradiente ou variação entre batches, possivelmente por batch size pequeno ou alta variance nos exemplos.

---

### Resultados Quantitativos

- BLEU
  - Pré‑treino: 0.0000
  - Fine‑tuned: 0.0007
- ROUGE
  - ROUGE‑1: 0.1311 → 0.1659 (melhora relativa significativa)
  - ROUGE‑2: 0.0739 → 0.0706 (leve queda)
  - ROUGE‑L: 0.1116 → 0.1332 (melhora)
  - ROUGE‑Lsum: 0.1122 → 0.1328 (melhora)

---

### Interpretação dos Resultados

- A melhora em ROUGE‑1 e ROUGE‑L indica que o fine‑tuning ajudou o modelo a recuperar mais unigramas e sequência mais longa do texto de referência, o que normalmente traduz maior fidelidade lexical e estrutural.  
- A quase ausência de BLEU e a queda sutil em ROUGE‑2 sugerem que o modelo melhorou o conteúdo geral e a fluidez, mas não capturou com precisão bigramas ou padrões mais finos. Isso é comum quando referências têm alta diversidade ou quando ferimos o comprimento/contexto por truncamento.  
- A magnitude das melhorias é modesta: o fine‑tuning trouxe ganho mensurável, mas não transformador. Para tarefas sensíveis à ordem exata de tokens, ganhos maiores em ROUGE‑2 e BLEU seriam desejáveis.

---

### Possíveis Limitações e Causas

- Single epoch: 1 passagem é insuficiente; muitos ganhos aparecem entre 2–6 épocas dependendo do dataset.  
- Max target length e truncamento: se referências longas foram truncadas, sinais críticos podem ter sido perdidos, prejudicando n‑gram capture.  
- Decoding config: max_new_tokens=272 limita comprimento das respostas; mismatch entre tamanho alvo e geração pode reduzir ROUGE‑2/BLEU.  
- Tokenizer / Preprocess: se labels foram truncados ou normalizados de forma agressiva, métricas n‑gram podem ser afetadas.  
- Metric sensitivity: BLEU tende a ser muito punitivo em tarefas abertas; valores muito baixos não necessariamente significam saída inútil.  
- Hiperparâmetros: lr, batch size, LoRA rank e dropout podem estar subótimos para extrair mais do dataset.

---

### Recomendações Práticas

1. Rodar mais épocas com validação periódica  
   - Objetivo: verificar tendência de validação e evitar overfit; sugerir 3–5 épocas com early stopping.  
2. Aumentar max_target_length e alinhar max_new_tokens  
   - Objetivo: reduzir truncamento de referências importantes; testar 320 → 441 → 512 conforme VRAM.  
3. Experimentos de learning rate e LoRA  
   - Sweep simples: lr ∈ {3e‑5, 1e‑4, 3e‑4}, r ∈ {8,16,32}. Incluir sanity overfit em 16–32 exemplos.  
4. Ajustar decoding  
   - Testar beam sizes e penalties; gerar com várias seeds para estabilidade; comparar métricas e amostra humana.  
5. Avaliação humana e exemplos qualitativos  
   - Colher 100 exemplos críticos, comparar pré/pos fine‑tune qualitativamente para evidenciar melhorias reais.  
6. Medidas adicionais  
   - Calcular BERTScore e métricas de factualidade se aplicável; reportar tradeoffs entre fluidez e fidelidade.  
7. Reduzir ruído do treino  
   - Aumentar effective batch size via gradient accumulation para gradientes mais estáveis.

---

### Resumo para apresentação em aula

- Objetivo: adaptar modelo base ao domínio com 50k exemplos usando LoRA/seq2seq fine‑tuning.  
- Setup: 1 época, training loss caiu de 3.44 para ~3.22, treinamento estável.  
- Resultado: melhoria em ROUGE‑1 e ROUGE‑L, ROUGE‑2 estável, BLEU quase zero (0.0007) — ganho modesto, mensurável.  
- Interpretação curta: fine‑tuning trouxe aumento de recuperação léxica e sequência, mas não melhorou fortemente n‑gramas mais longos; recomenda‑se continuar treinando e ajustar truncamento/decoding.  
- Próximos passos propostos: mais épocas com validação, sweep de lr/r, aumentar max_target_length, e avaliação humana em amostras representativas.

Se quiser, eu monto:
- um slide condensado com os pontos chave para sua apresentação, ou  
- um bloco de código para rodar os experimentos sugeridos (sweep de lr/r e alteração de max_target_length).  

Qual prefere?
